{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SecretPasta/DAGFCN/blob/main/Final_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z-IjW5iWDv5j"
      },
      "source": [
        "# Prerequisites\n",
        "Clean installing pytorch depenedencies for Node2Vec, and doing Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_ptX90PqAoPj"
      },
      "outputs": [],
      "source": [
        "!pip uninstall -y torch-scatter torch-sparse torch-cluster torch-spline-conv\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rkLyGwH5Arus"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "print(torch.__version__)  # Should match the version in the installation instructions\n",
        "print(torch.version.cuda)  # Ensure this matches the target version (e.g., '11.8')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0W497zhfqYLH"
      },
      "outputs": [],
      "source": [
        "# Required installations for PyTorch Geometric\n",
        "!pip install -q torch-scatter torch-sparse torch-cluster torch-spline-conv torch-geometric -f https://data.pyg.org/whl/torch-2.5.1+cu121.html\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x2cVZ31UBtpk"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "    import torch_scatter\n",
        "    import torch_sparse\n",
        "    import torch_cluster\n",
        "    import torch_spline_conv\n",
        "    print(\"All required libraries are installed and working!\")\n",
        "except ImportError as e:\n",
        "    print(f\"Error: {e}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K8oAkfbaA6xO"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch_geometric.nn import Node2Vec\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA version: {torch.version.cuda}\")\n",
        "print(\"PyTorch Geometric is successfully installed!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SBuwTE68qYIq"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch_geometric.datasets import CoraFull\n",
        "from torch_geometric.nn import Node2Vec\n",
        "from torch_geometric.utils import to_torch_csr_tensor\n",
        "from sklearn.ensemble import IsolationForest\n",
        "from sklearn.decomposition import PCA\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "print(torch.cuda.is_available())  # Should return True\n",
        "print(torch.cuda.get_device_name(0))  # Should display T4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lWG090XtqYGb"
      },
      "outputs": [],
      "source": [
        "# Check device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hl_CYE9BEEbN"
      },
      "source": [
        "# Datasets\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M6-9cR6NZdFT",
        "outputId": "368cc891-835e-4f66-a584-d1a1f9fbd5d3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lpHAhR4B2uZc"
      },
      "source": [
        "Loading CoraFull dataset from torch_geometric"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xfkPghvPqYDy"
      },
      "outputs": [],
      "source": [
        "def load_cora_dataset(save_path=\"/content/drive/My Drive/Dataset/CoraFull_saved.pt\"):\n",
        "\n",
        "    # Ensure the directory exists\n",
        "    os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
        "\n",
        "    # Load the dataset\n",
        "    dataset = CoraFull(root='./data/CoraFull')\n",
        "    data = dataset[0].to(device)\n",
        "\n",
        "    # Calculate the memory usage of the dataset in bytes\n",
        "    memory_usage = 0\n",
        "    for key, value in data:\n",
        "        if isinstance(value, torch.Tensor):\n",
        "            memory_usage += value.element_size() * value.numel()\n",
        "\n",
        "    # Convert memory usage to GB\n",
        "    memory_usage_gb = memory_usage / (1024 ** 3)\n",
        "\n",
        "    print(f\"Dataset: {dataset}\")\n",
        "    print(f\"Number of nodes: {data.num_nodes}\")\n",
        "    print(f\"Number of edges: {data.num_edges}\")\n",
        "    print(f\"Number of features: {data.num_features}\")\n",
        "    print(f\"Number of classes: {dataset.num_classes}\")\n",
        "    print(f\"Dataset size: {memory_usage_gb:.4f} GB\")\n",
        "\n",
        "    # Save the dataset to the specified path in Google Drive\n",
        "    torch.save(data, save_path)\n",
        "    print(f\"Dataset saved to: {save_path}\")\n",
        "\n",
        "    return data\n",
        "\n",
        "# Specify the save path in Google Drive\n",
        "save_path = \"/content/drive/My Drive/Dataset/CoraFull_saved.pt\"\n",
        "\n",
        "# Load and save the dataset\n",
        "data = load_cora_dataset(save_path=save_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cAY4sSJe2vwX"
      },
      "source": [
        "Loading Pubmed dataset from torch_geometric"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PgKQIWjiaW6M"
      },
      "outputs": [],
      "source": [
        "from torch_geometric.datasets import Planetoid\n",
        "\n",
        "def load_and_save_pubmed(save_path=\"/content/drive/My Drive/Dataset/Pubmed_saved.pt\"):\n",
        "    \"\"\"\n",
        "    Load the Pubmed dataset, print its statistics, and save it to Google Drive.\n",
        "\n",
        "    Parameters:\n",
        "        save_path (str): The file path to save the dataset in Google Drive.\n",
        "\n",
        "    Returns:\n",
        "        torch_geometric.data.Data: The loaded dataset.\n",
        "    \"\"\"\n",
        "    # Ensure the directory exists\n",
        "    os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
        "\n",
        "    # Load the Pubmed dataset\n",
        "    dataset = Planetoid(root='./data/Pubmed', name='Pubmed')\n",
        "    data = dataset[0].to(device)\n",
        "\n",
        "    # Calculate the memory usage of the dataset in bytes\n",
        "    memory_usage = 0\n",
        "    for key, value in data:\n",
        "        if isinstance(value, torch.Tensor):\n",
        "            memory_usage += value.element_size() * value.numel()\n",
        "\n",
        "    # Convert memory usage to GB\n",
        "    memory_usage_gb = memory_usage / (1024 ** 3)\n",
        "\n",
        "    print(f\"Dataset: {dataset}\")\n",
        "    print(f\"Number of nodes: {data.num_nodes}\")\n",
        "    print(f\"Number of edges: {data.num_edges}\")\n",
        "    print(f\"Number of features: {data.num_features}\")\n",
        "    print(f\"Number of classes: {dataset.num_classes}\")\n",
        "    print(f\"Dataset size: {memory_usage_gb:.4f} GB\")\n",
        "\n",
        "    # Save the dataset to the specified path in Google Drive\n",
        "    torch.save(data, save_path)\n",
        "    print(f\"Dataset saved to: {save_path}\")\n",
        "\n",
        "    return data\n",
        "\n",
        "# Specify the save path in Google Drive\n",
        "save_path = \"/content/drive/My Drive/Dataset/Pubmed_saved.pt\"\n",
        "\n",
        "# Load and save the Pubmed dataset\n",
        "#data = load_and_save_pubmed(save_path=save_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loading CiteSeer from torch.geometric"
      ],
      "metadata": {
        "id": "mWpWIXPE7BLT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_and_save_citeseer(save_path=\"/content/drive/My Drive/Dataset/CiteSeer_saved.pt\"):\n",
        "    \"\"\"\n",
        "    Load the Pubmed dataset, print its statistics, and save it to Google Drive.\n",
        "\n",
        "    Parameters:\n",
        "        save_path (str): The file path to save the dataset in Google Drive.\n",
        "\n",
        "    Returns:\n",
        "        torch_geometric.data.Data: The loaded dataset.\n",
        "    \"\"\"\n",
        "    # Ensure the directory exists\n",
        "    os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
        "\n",
        "    # Load the Pubmed dataset\n",
        "    dataset = Planetoid(root='./data/CiteSeer', name='CiteSeer')\n",
        "    data = dataset[0].to(device)\n",
        "\n",
        "    # Calculate the memory usage of the dataset in bytes\n",
        "    memory_usage = 0\n",
        "    for key, value in data:\n",
        "        if isinstance(value, torch.Tensor):\n",
        "            memory_usage += value.element_size() * value.numel()\n",
        "\n",
        "    # Convert memory usage to GB\n",
        "    memory_usage_gb = memory_usage / (1024 ** 3)\n",
        "\n",
        "    print(f\"Dataset: {dataset}\")\n",
        "    print(f\"Number of nodes: {data.num_nodes}\")\n",
        "    print(f\"Number of edges: {data.num_edges}\")\n",
        "    print(f\"Number of features: {data.num_features}\")\n",
        "    print(f\"Number of classes: {dataset.num_classes}\")\n",
        "    print(f\"Dataset size: {memory_usage_gb:.4f} GB\")\n",
        "\n",
        "    # Save the dataset to the specified path in Google Drive\n",
        "    torch.save(data, save_path)\n",
        "    print(f\"Dataset saved to: {save_path}\")\n",
        "\n",
        "    return data\n",
        "\n",
        "# Specify the save path in Google Drive\n",
        "save_path = \"/content/drive/My Drive/Dataset/CiteSeer_saved.pt\"\n",
        "\n",
        "# Load and save the Pubmed dataset\n",
        "#data = load_and_save_citeseer(save_path=save_path)"
      ],
      "metadata": {
        "id": "dX2M6XGX2-xU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SauxG-2z2zSS"
      },
      "source": [
        "Load the datasets from drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yjTE4oo1bXZ_"
      },
      "outputs": [],
      "source": [
        "def load_cora_from_drive(load_path=\"/content/drive/My Drive/Dataset/CoraFull_saved.pt\"):\n",
        "    \"\"\"\n",
        "    Load the saved Cora dataset from Google Drive.\n",
        "\n",
        "    Parameters:\n",
        "        load_path (str): The file path to load the Cora dataset from Google Drive.\n",
        "\n",
        "    Returns:\n",
        "        torch_geometric.data.Data: The loaded Cora dataset.\n",
        "    \"\"\"\n",
        "    if not os.path.exists(load_path):\n",
        "        raise FileNotFoundError(f\"The file at {load_path} does not exist. Please ensure it is saved correctly.\")\n",
        "\n",
        "    data = torch.load(load_path)\n",
        "    print(f\"Cora dataset loaded successfully from {load_path}.\")\n",
        "    print(f\"Number of nodes: {data.num_nodes}\")\n",
        "    print(f\"Number of edges: {data.num_edges}\")\n",
        "    print(f\"Number of features: {data.num_features}\")\n",
        "    return data\n",
        "#data = load_cora_from_drive()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MLhGD16YbXTd"
      },
      "outputs": [],
      "source": [
        "def load_pubmed_from_drive(load_path=\"/content/drive/My Drive/Dataset/Pubmed_saved.pt\"):\n",
        "    \"\"\"\n",
        "    Load the saved Pubmed dataset from Google Drive.\n",
        "\n",
        "    Parameters:\n",
        "        load_path (str): The file path to load the Pubmed dataset from Google Drive.\n",
        "\n",
        "    Returns:\n",
        "        torch_geometric.data.Data: The loaded Pubmed dataset.\n",
        "    \"\"\"\n",
        "    if not os.path.exists(load_path):\n",
        "        raise FileNotFoundError(f\"The file at {load_path} does not exist. Please ensure it is saved correctly.\")\n",
        "\n",
        "    data = torch.load(load_path)\n",
        "    print(f\"Pubmed dataset loaded successfully from {load_path}.\")\n",
        "    print(f\"Number of nodes: {data.num_nodes}\")\n",
        "    print(f\"Number of edges: {data.num_edges}\")\n",
        "    print(f\"Number of features: {data.num_features}\")\n",
        "    return data\n",
        "\n",
        "#data = load_pubmed_from_drive()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_citeseer_from_drive(load_path=\"/content/drive/My Drive/Dataset/CiteSeer_saved.pt\"):\n",
        "    \"\"\"\n",
        "    Load the saved CiteSeer dataset from Google Drive.\n",
        "\n",
        "    Parameters:\n",
        "        load_path (str): The file path to load the CiteSeer dataset from Google Drive.\n",
        "\n",
        "    Returns:\n",
        "        torch_geometric.data.Data: The loaded CiteSeer dataset.\n",
        "    \"\"\"\n",
        "    if not os.path.exists(load_path):\n",
        "        raise FileNotFoundError(f\"The file at {load_path} does not exist. Please ensure it is saved correctly.\")\n",
        "\n",
        "    data = torch.load(load_path)\n",
        "    print(f\"CiteSeer dataset loaded successfully from {load_path}.\")\n",
        "    print(f\"Number of nodes: {data.num_nodes}\")\n",
        "    print(f\"Number of edges: {data.num_edges}\")\n",
        "    print(f\"Number of features: {data.num_features}\")\n",
        "    return data\n",
        "\n",
        "#data = load_citeseer_from_drive()"
      ],
      "metadata": {
        "id": "2gU5Sx2S3Qgq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FsUpRlrHERzf"
      },
      "source": [
        "# Node2Vec\n",
        "\n",
        "Passing in the dataset to generate node embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z1DDkT46CuOv"
      },
      "outputs": [],
      "source": [
        "embedding_dim = 128\n",
        "walk_length = 20\n",
        "context_size = 10\n",
        "walks_per_node=10\n",
        "num_negative_samples=1\n",
        "p=0.5\n",
        "q=0.25\n",
        "sparse=True\n",
        "n2v_lr=0.01 #learning rate\n",
        "n2v_bs=128 #batch size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DJmfZ4OVqYAw"
      },
      "outputs": [],
      "source": [
        "# Step 1: Node2Vec for Embedding Initialization\n",
        "node2vec = Node2Vec(\n",
        "    edge_index=data.edge_index,\n",
        "    embedding_dim=embedding_dim,\n",
        "    walk_length=walk_length,\n",
        "    context_size=context_size,\n",
        "    walks_per_node=walks_per_node,\n",
        "    num_negative_samples=num_negative_samples,\n",
        "    p=p, q=q,\n",
        "    sparse=sparse\n",
        ").to(device)\n",
        "\n",
        "node2vec_optimizer = torch.optim.SparseAdam(node2vec.parameters(), lr=n2v_lr)\n",
        "node2vec_loader = node2vec.loader(batch_size=n2v_bs, shuffle=True)\n",
        "\n",
        "def train_node2vec(epochs=10):\n",
        "    node2vec.train()\n",
        "    for epoch in range(epochs):\n",
        "        total_loss = 0\n",
        "        for pos_rw, neg_rw in node2vec_loader:\n",
        "            node2vec_optimizer.zero_grad()\n",
        "            loss = node2vec.loss(pos_rw.to(device), neg_rw.to(device))\n",
        "            loss.backward()\n",
        "            node2vec_optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "        print(f\"Epoch {epoch + 1}/{epochs}, Loss: {total_loss / len(node2vec_loader):.4f}\")\n",
        "\n",
        "train_node2vec(epochs=10)\n",
        "\n",
        "# Extract embeddings\n",
        "node_embeddings = node2vec().detach().cpu().numpy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W4G8jl30E3Qj"
      },
      "source": [
        "# Isolation Forest\n",
        "\n",
        "Passing in the Node Embeddings into the Isolation forest to isolate anamolies within the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v8Au4MYYE_ql"
      },
      "outputs": [],
      "source": [
        "def isolation_forest(node_embeddings, n_estimators=50, contamination=0.2):\n",
        "    \"\"\"\n",
        "    Detect anomalies in node embeddings using Isolation Forest.\n",
        "\n",
        "    Parameters:\n",
        "        node_embeddings (numpy.ndarray): A 2D array where each row represents the embedding of a node.\n",
        "        n_estimators (int): Number of trees in the Isolation Forest. Default is 100.\n",
        "        contamination (float): The proportion of anomalies in the data. Default is 0.1.\n",
        "\n",
        "    Returns:\n",
        "        tuple:\n",
        "            numpy.ndarray: Labels array where 1 indicates an anomaly and 0 indicates normal.\n",
        "            numpy.ndarray: Anomaly mask where True indicates an anomaly and False indicates normal.\n",
        "    \"\"\"\n",
        "    # Initialize the Isolation Forest model\n",
        "    isolation_model = IsolationForest(\n",
        "        n_estimators=n_estimators,\n",
        "        contamination=contamination,\n",
        "        random_state=42\n",
        "    )\n",
        "\n",
        "    # Fit the model to the node embeddings\n",
        "    isolation_model.fit(node_embeddings)\n",
        "\n",
        "    # Predict anomaly labels: 1 for normal, -1 for anomaly\n",
        "    labels = isolation_model.predict(node_embeddings)\n",
        "\n",
        "    # Create an anomaly mask: True for anomalies, False for normal points\n",
        "    anomaly_mask = labels == -1\n",
        "\n",
        "    # Adjust labels to binary format: -1 (anomaly) -> 1, 1 (normal) -> 0\n",
        "    labels = anomaly_mask.astype(int)\n",
        "\n",
        "    return labels, anomaly_mask"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3MHcQkuvFAUC"
      },
      "source": [
        "# GFCN\n",
        "\n",
        "Defining the Graph Fairing Convolutional Network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eDn2mcddtxZx"
      },
      "outputs": [],
      "source": [
        "# Step 3: Graph Fairing Convolutional Network (GFCN)\n",
        "class GraphConvolution(nn.Module):\n",
        "    def __init__(self, in_features, out_features, bias=True):\n",
        "        super(GraphConvolution, self).__init__()\n",
        "        self.weight = nn.Parameter(torch.FloatTensor(in_features, out_features))\n",
        "        self.bias = nn.Parameter(torch.FloatTensor(out_features)) if bias else None\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        nn.init.xavier_uniform_(self.weight)\n",
        "        if self.bias is not None:\n",
        "            nn.init.zeros_(self.bias)\n",
        "\n",
        "    def forward(self, x, adj):\n",
        "        support = torch.mm(x, self.weight)\n",
        "        output = torch.spmm(adj, support)\n",
        "        if self.bias is not None:\n",
        "            output += self.bias\n",
        "        return output\n",
        "\n",
        "class GFCN(nn.Module):\n",
        "    def __init__(self, nfeat, nhid, nclass, dropout):\n",
        "        super(GFCN, self).__init__()\n",
        "        self.gc1 = GraphConvolution(nfeat, nhid)\n",
        "        self.gc2 = GraphConvolution(nhid, nhid)\n",
        "        self.gc3 = GraphConvolution(nhid, nclass)\n",
        "        self.dropout = dropout\n",
        "\n",
        "    def forward(self, x, adj):\n",
        "        x = F.relu(self.gc1(x, adj))\n",
        "        x = F.dropout(x, self.dropout, training=self.training)\n",
        "        x = F.relu(self.gc2(x, adj))\n",
        "        x = F.dropout(x, self.dropout, training=self.training)\n",
        "        x = self.gc3(x, adj)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xo7s8MYFIYRT"
      },
      "outputs": [],
      "source": [
        "def gfcn(data, node_embeddings, anomaly_mask, device='cuda', epochs=150, train_ratio=0.8):\n",
        "    \"\"\"\n",
        "    Trains a Graph Fairing Convolutional Network (GFCN) on the given data and returns node labels.\n",
        "\n",
        "    Parameters:\n",
        "        data (torch_geometric.data.Data): The graph data containing edge_index and num_nodes.\n",
        "        node_embeddings (numpy.ndarray): Node embeddings as input features.\n",
        "        anomaly_mask (numpy.ndarray): Boolean mask indicating anomalies (True = anomaly).\n",
        "        device (str): Device to run the model on ('cuda' or 'cpu').\n",
        "        epochs (int): Number of training epochs. Default is 100.\n",
        "        train_ratio (float): Ratio of training nodes to total nodes. Default is 0.8.\n",
        "\n",
        "    Returns:\n",
        "        torch.Tensor: Predicted labels for all nodes (0 = normal, 1 = anomaly).\n",
        "    \"\"\"\n",
        "    # Convert node_embeddings and anomaly_mask to PyTorch tensors\n",
        "    features = torch.tensor(node_embeddings, dtype=torch.float32, device=device)\n",
        "    labels = torch.tensor(anomaly_mask.astype(int), dtype=torch.long, device=device)\n",
        "\n",
        "    # Convert edge_index to a PyTorch sparse tensor\n",
        "    adj = to_torch_csr_tensor(data.edge_index, size=(data.num_nodes, data.num_nodes)).to(device)\n",
        "\n",
        "    # Train/Test split\n",
        "    num_nodes = data.num_nodes\n",
        "    num_train = int(train_ratio * num_nodes)\n",
        "    idx_train = torch.arange(num_train, device=device)\n",
        "    idx_test = torch.arange(num_train, num_nodes, device=device)\n",
        "\n",
        "    # Define the GFCN model\n",
        "    class GFCN(nn.Module):\n",
        "        def __init__(self, nfeat, nhid, nclass, dropout):\n",
        "            super(GFCN, self).__init__()\n",
        "            self.gc1 = nn.Linear(nfeat, nhid)\n",
        "            self.gc2 = nn.Linear(nhid, nclass)\n",
        "            self.dropout = dropout\n",
        "\n",
        "        def forward(self, x, adj):\n",
        "            x = torch.relu(self.gc1(x))\n",
        "            x = torch.dropout(x, p=self.dropout, train=self.training)\n",
        "            x = self.gc2(x)\n",
        "            return x\n",
        "\n",
        "    # Model and optimizer\n",
        "    gcn = GFCN(nfeat=features.shape[1], nhid=64, nclass=2, dropout=0.5).to(device)\n",
        "    optimizer = torch.optim.Adam(gcn.parameters(), lr=0.01)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    # Training the model\n",
        "    def train_gfcn(epochs):\n",
        "        gcn.train()\n",
        "        for epoch in range(epochs):\n",
        "            optimizer.zero_grad()\n",
        "            output = gcn(features, adj)\n",
        "            loss = criterion(output[idx_train], labels[idx_train])\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            print(f\"Epoch {epoch + 1}/{epochs}, Loss: {loss.item():.4f}\")\n",
        "\n",
        "    train_gfcn(epochs)\n",
        "\n",
        "    # Evaluation: Predict labels for all nodes\n",
        "    gcn.eval()\n",
        "    with torch.no_grad():\n",
        "        output = gcn(features, adj)\n",
        "        predictions = torch.argmax(output, dim=1)  # Predicted labels\n",
        "\n",
        "    return predictions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OWXMrJK0BH1n"
      },
      "outputs": [],
      "source": [
        "def evaluate_model(data, node_embeddings, true_labels, predicted_labels, iteration):\n",
        "    \"\"\"\n",
        "    Evaluate the model at the end of each iteration.\n",
        "\n",
        "    Parameters:\n",
        "        data (torch_geometric.data.Data): The graph data containing edge_index and num_nodes.\n",
        "        node_embeddings (numpy.ndarray): Current node embeddings.\n",
        "        true_labels (numpy.ndarray): True labels of the nodes (1 for anomaly, 0 for normal).\n",
        "        predicted_labels (numpy.ndarray): Predicted labels from the GFCN (1 for anomaly, 0 for normal).\n",
        "        iteration (int): The current iteration number.\n",
        "\n",
        "    Returns:\n",
        "        dict: A dictionary containing evaluation metrics.\n",
        "    \"\"\"\n",
        "    print(f\"Evaluating model at iteration {iteration}...\")\n",
        "\n",
        "    # Calculate evaluation metrics manually\n",
        "    true_positives = np.sum((true_labels == 1) & (predicted_labels == 1))\n",
        "    true_negatives = np.sum((true_labels == 0) & (predicted_labels == 0))\n",
        "    false_positives = np.sum((true_labels == 0) & (predicted_labels == 1))\n",
        "    false_negatives = np.sum((true_labels == 1) & (predicted_labels == 0))\n",
        "\n",
        "    accuracy = (true_positives + true_negatives) / len(true_labels)\n",
        "    precision = true_positives / (true_positives + false_positives) if (true_positives + false_positives) > 0 else 0.0\n",
        "    recall = true_positives / (true_positives + false_negatives) if (true_positives + false_negatives) > 0 else 0.0\n",
        "    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0.0\n",
        "\n",
        "    print(f\"Iteration {iteration} - Evaluation Metrics:\")\n",
        "    print(f\"Accuracy: {accuracy:.4f}\")\n",
        "    print(f\"F1 Score: {f1:.4f}\")\n",
        "    print(f\"Precision: {precision:.4f}\")\n",
        "    print(f\"Recall: {recall:.4f}\")\n",
        "\n",
        "    return {\n",
        "        'accuracy': accuracy,\n",
        "        'f1': f1,\n",
        "        'precision': precision,\n",
        "        'recall': recall\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vX8UBAccBH8x"
      },
      "outputs": [],
      "source": [
        "def remove_nodes(data, node_indices):\n",
        "    \"\"\"\n",
        "    Remove nodes from graph data.\n",
        "\n",
        "    Parameters:\n",
        "        data (torch_geometric.data.Data): The graph data containing edge_index and num_nodes.\n",
        "        node_indices (numpy.ndarray): Indices of nodes to remove.\n",
        "\n",
        "    Returns:\n",
        "        torch_geometric.data.Data: Updated graph data with specified nodes removed.\n",
        "    \"\"\"\n",
        "    mask = np.ones(data.num_nodes, dtype=bool)\n",
        "    mask[node_indices[node_indices < data.num_nodes]] = False  # Ensure indices are within bounds\n",
        "\n",
        "    data.x = data.x[mask]\n",
        "\n",
        "    # Filter edges based on the updated node mask\n",
        "    edge_index_cpu = data.edge_index.cpu().numpy()\n",
        "    edge_index_cpu = edge_index_cpu[:, (edge_index_cpu[0] < mask.size) & (edge_index_cpu[1] < mask.size)]  # Ensure edge indices are within bounds\n",
        "    edge_mask = mask[edge_index_cpu[0]] & mask[edge_index_cpu[1]]\n",
        "    data.edge_index = torch.tensor(edge_index_cpu[:, edge_mask], dtype=torch.long, device=data.edge_index.device)\n",
        "\n",
        "    data.num_nodes = mask.sum()\n",
        "    return data"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "KXCz_Kzv7Pxw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creates a Folder for the experiments"
      ],
      "metadata": {
        "id": "LT-64kBz6b7e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to create a folder\n",
        "def create_folder_in_drive(base_path, folder_name):\n",
        "    \"\"\"\n",
        "    Create a folder in a specified path in Google Drive.\n",
        "\n",
        "    :param base_path: The base path where the folder will be created (str)\n",
        "    :param folder_name: The name of the folder to create (str)\n",
        "    :return: Full path of the created folder (str)\n",
        "    \"\"\"\n",
        "    # Combine base path and folder name\n",
        "    folder_path = os.path.join(base_path, folder_name)\n",
        "\n",
        "    # Create the folder if it doesn't exist\n",
        "    if not os.path.exists(folder_path):\n",
        "        os.makedirs(folder_path)\n",
        "        print(f\"Folder created: {folder_path}\")\n",
        "    else:\n",
        "        print(f\"Folder already exists: {folder_path}\")\n",
        "\n",
        "    return folder_path\n",
        "\n",
        "# Path to the folder holding the experiments\n",
        "base_path = '/content/drive/MyDrive/Final Project - Boris & Omri/Experiments'\n",
        "\n",
        "# Specify the folder name should be experiment name\n",
        "folder_name = 'TestRun'\n",
        "\n",
        "# Create the folder\n",
        "output_folder  = create_folder_in_drive(base_path, folder_name)\n",
        "print(f\"Folder path: {output_folder }\")\n"
      ],
      "metadata": {
        "id": "ce3hPEVq6eiv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "INIb4MVnKG2m"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "def iterative_anomaly_detection(data, node_embeddings, K, device='cuda'):\n",
        "    evaluation_scores = []\n",
        "    removed_nodes_per_iteration = []  # Track nodes removed in each iteration\n",
        "\n",
        "    print(f\"Initial Number of Nodes: {data.num_nodes}\")\n",
        "    for iteration in range(K):\n",
        "        # Step 1: Apply Isolation Forest to detect anomalies\n",
        "        labels, anomaly_mask = isolation_forest(node_embeddings, 150, 0.3)\n",
        "        num_anomalies_iforest = np.sum(anomaly_mask)\n",
        "        print(f\"Iteration {iteration + 1}/{K}: {num_anomalies_iforest} anomalies detected by Isolation Forest.\")\n",
        "\n",
        "        # Step 2: Apply GFCN to classify anomalies\n",
        "        predictions = gfcn(data, node_embeddings, anomaly_mask, device=device)\n",
        "        anomaly_indices = np.where(predictions.cpu().numpy() == 1)[0]\n",
        "        num_anomalies_gfcn = len(anomaly_indices)\n",
        "        print(f\"Iteration {iteration + 1}/{K}: {num_anomalies_gfcn} anomalies detected by GFCN.\")\n",
        "\n",
        "        if num_anomalies_gfcn == 0:\n",
        "            print(\"No anomalies detected by GFCN. Stopping iteration.\")\n",
        "            break\n",
        "\n",
        "        anomaly_indices = anomaly_indices[anomaly_indices < node_embeddings.shape[0]]\n",
        "        removed_nodes_per_iteration.append(anomaly_indices.tolist())\n",
        "        node_embeddings = np.delete(node_embeddings, anomaly_indices, axis=0)\n",
        "        data = remove_nodes(data, anomaly_indices)\n",
        "        print(f\"Iteration {iteration + 1}/{K}: Number of Nodes after anomaly removal: {data.num_nodes}\")\n",
        "\n",
        "        scores = evaluate_model(data, node_embeddings, labels, predictions.cpu().numpy(), iteration + 1)\n",
        "        evaluation_scores.append(scores)\n",
        "\n",
        "    # Plot the final evaluation scores\n",
        "    iterations = range(1, len(evaluation_scores) + 1)\n",
        "    accuracies = [score[\"accuracy\"] for score in evaluation_scores]\n",
        "    f1_scores = [score[\"f1\"] for score in evaluation_scores]\n",
        "    precisions = [score[\"precision\"] for score in evaluation_scores]\n",
        "    recalls = [score[\"recall\"] for score in evaluation_scores]\n",
        "\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(iterations, accuracies, label='Accuracy', marker='o')\n",
        "    plt.plot(iterations, f1_scores, label='F1 Score', marker='o')\n",
        "    plt.plot(iterations, precisions, label='Precision', marker='o')\n",
        "    plt.plot(iterations, recalls, label='Recall', marker='o')\n",
        "\n",
        "    plt.xlabel('Iteration')\n",
        "    plt.ylabel('Score')\n",
        "    plt.title('Evaluation Scores Across Iterations')\n",
        "    plt.xticks(iterations)\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "\n",
        "    # Save the plot to the output folder\n",
        "    plot_path = os.path.join(output_folder, 'evaluation_scores_plot.png')\n",
        "    plt.savefig(plot_path)\n",
        "    print(f\"Plot saved to {plot_path}\")\n",
        "    plt.show()\n",
        "\n",
        "    # Save removed nodes per iteration to a CSV file\n",
        "    csv_path = os.path.join(output_folder, 'removed_nodes_per_iteration.csv')\n",
        "    removed_nodes_df = pd.DataFrame({'Iteration': range(1, len(removed_nodes_per_iteration) + 1),\n",
        "                                     'RemovedNodes': removed_nodes_per_iteration})\n",
        "    removed_nodes_df.to_csv(csv_path, index=False)\n",
        "    print(f\"Removed nodes saved to {csv_path}\")\n",
        "\n",
        "    # Print final evaluation scores\n",
        "    print(\"\\nFinal Evaluation Scores:\")\n",
        "    for i, scores in enumerate(evaluation_scores, 1):\n",
        "        print(f\"Iteration {i} - Accuracy: {scores['accuracy']:.4f}, F1 Score: {scores['f1']:.4f}, Precision: {scores['precision']:.4f}, Recall: {scores['recall']:.4f}\")\n",
        "\n",
        "    return node_embeddings, removed_nodes_per_iteration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iwamUqhKKJsr"
      },
      "outputs": [],
      "source": [
        "#print(f\"Initial Number of Nodes: {cora_data.num_nodes}\")\n",
        "#node_embeddings_final, removed_nodes=iterative_anomaly_detection(load_cora_dataset(),node_embeddings, 10)\n",
        "node_embeddings_final, removed_nodes=iterative_anomaly_detection(data,node_embeddings, 10)\n",
        "#node_embeddings_final, removed_nodes=iterative_anomaly_detection(load_citeseer_from_drive(),node_embeddings, 50)\n",
        "#node_embeddings_final, removed_nodes=iterative_anomaly_detection(load_snap_from_drive(),node_embeddings, 10)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "def plot_remaining_and_removed_embeddings(final_embeddings, removed_nodes, original_embeddings, output_folder):\n",
        "    \"\"\"\n",
        "    Plots the remaining and removed node embeddings in two different colors with enhanced visibility,\n",
        "    with PCA applied separately for each group, and saves the plot as an image.\n",
        "\n",
        "    Parameters:\n",
        "        final_embeddings (np.ndarray): Array of final embeddings, shape (m, d).\n",
        "        removed_nodes (list): List of lists containing indices of removed nodes per iteration.\n",
        "        original_embeddings (np.ndarray): Array of original embeddings, shape (n, d).\n",
        "        output_folder (str): Path to the folder where the plot image will be saved.\n",
        "    \"\"\"\n",
        "    import os\n",
        "    import numpy as np\n",
        "    import matplotlib.pyplot as plt\n",
        "    from sklearn.decomposition import PCA\n",
        "\n",
        "    # Ensure inputs are NumPy arrays\n",
        "    final_embeddings = np.array(final_embeddings)\n",
        "    original_embeddings = np.array(original_embeddings)\n",
        "\n",
        "    # Extract embeddings for removed nodes\n",
        "    removed_indices = [idx for iteration in removed_nodes for idx in iteration]\n",
        "    removed_embeddings = original_embeddings[removed_indices]\n",
        "\n",
        "    # Perform PCA separately for remaining and removed nodes\n",
        "    pca_final = PCA(n_components=2)\n",
        "    reduced_final = pca_final.fit_transform(final_embeddings)\n",
        "\n",
        "    pca_removed = PCA(n_components=2)\n",
        "    reduced_removed = pca_removed.fit_transform(removed_embeddings)\n",
        "\n",
        "    # Plot the reduced embeddings\n",
        "    plt.figure(figsize=(12, 8))\n",
        "    plt.scatter(\n",
        "        reduced_final[:, 0],\n",
        "        reduced_final[:, 1],\n",
        "        color='green',\n",
        "        marker='o',\n",
        "        s=50,\n",
        "        label='Remaining Embeddings',\n",
        "        alpha=0.8,\n",
        "        edgecolor='black'\n",
        "    )\n",
        "    plt.scatter(\n",
        "        reduced_removed[:, 0],\n",
        "        reduced_removed[:, 1],\n",
        "        color='red',\n",
        "        marker='x',\n",
        "        s=70,\n",
        "        label='Removed Embeddings',\n",
        "        alpha=0.8\n",
        "    )\n",
        "\n",
        "    # Add labels, legend, and title\n",
        "    plt.xlabel(\"\")\n",
        "    plt.ylabel(\"\")\n",
        "    plt.legend(fontsize=12)\n",
        "    plt.title(\"Node Embeddings: Remaining vs Removed\", fontsize=14)\n",
        "    plt.grid(True, linestyle='--', alpha=0.6)\n",
        "\n",
        "    # Save the plot\n",
        "    output_path = os.path.join(output_folder, \"CoraFull.png\")\n",
        "    plt.savefig(output_path, dpi=300, bbox_inches='tight')\n",
        "\n",
        "    # Display the plot\n",
        "    plt.show()\n",
        "\n",
        "    print(f\"Plot saved to {output_path}\")\n",
        "\n",
        "plot_remaining_and_removed_embeddings(node_embeddings_final, removed_nodes, node_embeddings,output_folder)"
      ],
      "metadata": {
        "id": "CYtMKqJYEhsG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.decomposition import PCA\n",
        "import plotly.graph_objects as go\n",
        "\n",
        "def plot_remaining_and_removed_embeddings_plotly(final_embeddings, removed_nodes, original_embeddings):\n",
        "    \"\"\"\n",
        "    Plots the remaining and removed node embeddings in an interactive 3D plot using Plotly,\n",
        "    with PCA applied separately for each group.\n",
        "\n",
        "    Parameters:\n",
        "        final_embeddings (np.ndarray): Array of final embeddings, shape (m, d).\n",
        "        removed_nodes (list): List of lists containing indices of removed nodes per iteration.\n",
        "        original_embeddings (np.ndarray): Array of original embeddings, shape (n, d).\n",
        "    \"\"\"\n",
        "    # Ensure inputs are NumPy arrays\n",
        "    final_embeddings = np.array(final_embeddings)\n",
        "    original_embeddings = np.array(original_embeddings)\n",
        "\n",
        "    # Extract embeddings for removed nodes\n",
        "    removed_indices = [idx for iteration in removed_nodes for idx in iteration]\n",
        "    removed_embeddings = original_embeddings[removed_indices]\n",
        "\n",
        "    # Perform PCA separately for remaining and removed nodes\n",
        "    pca_final = PCA(n_components=3)\n",
        "    reduced_final = pca_final.fit_transform(final_embeddings)\n",
        "\n",
        "    pca_removed = PCA(n_components=3)\n",
        "    reduced_removed = pca_removed.fit_transform(removed_embeddings)\n",
        "\n",
        "    # Create a Plotly 3D scatter plot\n",
        "    fig = go.Figure()\n",
        "\n",
        "    # Add remaining nodes\n",
        "    fig.add_trace(go.Scatter3d(\n",
        "        x=reduced_final[:, 0],\n",
        "        y=reduced_final[:, 1],\n",
        "        z=reduced_final[:, 2],\n",
        "        mode='markers',\n",
        "        marker=dict(size=5, color='green', opacity=0.8),\n",
        "        name='Remaining Embeddings'\n",
        "    ))\n",
        "\n",
        "    # Add removed nodes\n",
        "    fig.add_trace(go.Scatter3d(\n",
        "        x=reduced_removed[:, 0],\n",
        "        y=reduced_removed[:, 1],\n",
        "        z=reduced_removed[:, 2],\n",
        "        mode='markers',\n",
        "        marker=dict(size=6, color='red', opacity=0.8),\n",
        "        name='Removed Embeddings'\n",
        "    ))\n",
        "\n",
        "    # Update layout\n",
        "    fig.update_layout(\n",
        "        scene=dict(\n",
        "            xaxis_title='PCA Dimension 1',\n",
        "            yaxis_title='PCA Dimension 2',\n",
        "            zaxis_title='PCA Dimension 3',\n",
        "        ),\n",
        "        title=\"Interactive 3D Node Embeddings\",\n",
        "        legend=dict(font=dict(size=10)),\n",
        "        margin=dict(l=0, r=0, b=0, t=40)\n",
        "    )\n",
        "\n",
        "    # Show the plot\n",
        "    fig.show()\n",
        "\n",
        "plot_remaining_and_removed_embeddings_plotly(node_embeddings_final, removed_nodes, node_embeddings)\n"
      ],
      "metadata": {
        "id": "qPX53rAR8HYe"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}