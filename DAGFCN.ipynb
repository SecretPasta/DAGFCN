{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPfc4AC6EyZQY9YW+x6LK/Q",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SecretPasta/DAGFCN/blob/main/DAGFCN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Imports"
      ],
      "metadata": {
        "id": "bBsxBt1yYFbW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CiovlEG2XP7w",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7c890965-dbc9-4e00-d111-27176a8ef56a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from __future__ import print_function\n",
        "from __future__ import division\n",
        "\n",
        "#from .layers import *\n",
        "#from .models import *\n",
        "#from .utils import *\n",
        "\n",
        "import math\n",
        "\n",
        "import torch\n",
        "\n",
        "from torch.nn.parameter import Parameter\n",
        "from torch.nn.modules.module import Module\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "#from layers import GraphConvolution\n",
        "\n",
        "import time\n",
        "import argparse\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "#from utils import load_data\n",
        "#from models import GCN\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "\n",
        "import numpy as np\n",
        "import pickle as pkl\n",
        "import networkx as nx\n",
        "import scipy.sparse as sp\n",
        "import sys\n",
        "import torch\n",
        "import sklearn\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Layers"
      ],
      "metadata": {
        "id": "khX2mhw1YLMn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GraphConvolution(Module):\n",
        "    \"\"\"\n",
        "    Simple GCN layer, similar to https://arxiv.org/abs/1609.02907\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, in_features, out_features, bias=True):\n",
        "        super(GraphConvolution, self).__init__()\n",
        "        self.in_features = in_features\n",
        "        self.out_features = out_features\n",
        "        self.weight = Parameter(torch.FloatTensor(in_features, out_features))\n",
        "        if bias:\n",
        "            self.bias = Parameter(torch.FloatTensor(out_features))\n",
        "        else:\n",
        "            self.register_parameter('bias', None)\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        stdv = 1. / math.sqrt(self.weight.size(1))\n",
        "        self.weight.data.uniform_(-stdv, stdv)\n",
        "        if self.bias is not None:\n",
        "            self.bias.data.uniform_(-stdv, stdv)\n",
        "\n",
        "    def forward(self, input, adj=None):\n",
        "        if adj is not None:\n",
        "            support = torch.mm(input, self.weight)\n",
        "            output = torch.spmm(adj, support)\n",
        "        else:\n",
        "                output = torch.mm(input, self.weight)\n",
        "        if self.bias is not None:\n",
        "                return output + self.bias\n",
        "        else:\n",
        "                return output\n",
        "\n",
        "\n",
        "    def __repr__(self):\n",
        "        return self.__class__.__name__ + ' (' \\\n",
        "               + str(self.in_features) + ' -> ' \\\n",
        "               + str(self.out_features) + ')'"
      ],
      "metadata": {
        "id": "_YbKOZazYEyv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Models"
      ],
      "metadata": {
        "id": "H6GxesdoYRO5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GCN(nn.Module):\n",
        "    def __init__(self, nfeat, nhid , nclass, dropout):\n",
        "        super(GCN, self).__init__()\n",
        "\n",
        "        self.gc1 = GraphConvolution(nfeat, nhid)\n",
        "        self.gc2 = GraphConvolution(nfeat, nhid)\n",
        "        self.gc3 = GraphConvolution(nhid, nclass)\n",
        "        self.gc4 = GraphConvolution(nfeat, nclass)\n",
        "        self.dropout = dropout\n",
        "\n",
        "\n",
        "    def forward(self, x, adj):\n",
        "\n",
        "        a_skip0 = self.gc2(x)\n",
        "        a_skip0 = F.dropout(a_skip0, self.dropout, training=self.training)\n",
        "        a_skip1 = self.gc4(x)\n",
        "        a_skip1 = F.dropout(a_skip1, self.dropout, training=self.training)\n",
        "\n",
        "        m = nn.RReLU()\n",
        "        x = m(self.gc1(x, adj) + a_skip0)\n",
        "        x = self.gc3(x, adj) + a_skip1\n",
        "\n",
        "        self.w1 = self.gc1.weight\n",
        "        self.w2 = self.gc2.weight\n",
        "        self.w3 = self.gc3.weight\n",
        "        self.w4 = self.gc4.weight\n",
        "\n",
        "        return x, self.w1, self.w2, self.w3, self.w4"
      ],
      "metadata": {
        "id": "UvN2vqQ7YSmf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Utils"
      ],
      "metadata": {
        "id": "V7zp68PPYkhq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def parse_index_file(filename):\n",
        "    \"\"\"Parse index file.\"\"\"\n",
        "    index = []\n",
        "    for line in open(filename):\n",
        "        index.append(int(line.strip()))\n",
        "    return index\n",
        "\n",
        "def sparse_mx_to_torch_sparse_tensor(sparse_mx):\n",
        "    \"\"\"Convert a scipy sparse matrix to a torch sparse tensor.\"\"\"\n",
        "    sparse_mx = sparse_mx.tocoo().astype(np.float32)\n",
        "    indices = torch.from_numpy(\n",
        "        np.vstack((sparse_mx.row, sparse_mx.col)).astype(np.int64))\n",
        "    values = torch.from_numpy(sparse_mx.data)\n",
        "    shape = torch.Size(sparse_mx.shape)\n",
        "    return torch.sparse.FloatTensor(indices, values, shape)\n",
        "\n",
        "def normalize(mx):\n",
        "    \"\"\"Row-normalize sparse matrix\"\"\"\n",
        "    rowsum = np.array(mx.sum(1))\n",
        "    r_inv = np.power(rowsum, -0.5).flatten()\n",
        "    r_inv[np.isinf(r_inv)] = 0.\n",
        "    r_mat_inv = sp.diags(r_inv)\n",
        "    mx1 = r_mat_inv.dot(mx)\n",
        "    mx = mx1.dot(r_mat_inv)\n",
        "    return mx\n",
        "\n",
        "def load_data(dataset_str):\n",
        "    \"\"\"\n",
        "    Loads input data from gcn/data directory\n",
        "\n",
        "    ind.dataset_str.x => the feature vectors of the training instances as scipy.sparse.csr.csr_matrix object;\n",
        "    ind.dataset_str.tx => the feature vectors of the test instances as scipy.sparse.csr.csr_matrix object;\n",
        "    ind.dataset_str.allx => the feature vectors of both labeled and unlabeled training instances\n",
        "        (a superset of ind.dataset_str.x) as scipy.sparse.csr.csr_matrix object;\n",
        "    ind.dataset_str.y => the one-hot labels of the labeled training instances as numpy.ndarray object;\n",
        "    ind.dataset_str.ty => the one-hot labels of the test instances as numpy.ndarray object;\n",
        "    ind.dataset_str.ally => the labels for instances in ind.dataset_str.allx as numpy.ndarray object;\n",
        "    ind.dataset_str.graph => a dict in the format {index: [index_of_neighbor_nodes]} as collections.defaultdict\n",
        "        object;\n",
        "    ind.dataset_str.test.index => the indices of test instances in graph, for the inductive setting as list object.\n",
        "\n",
        "    All objects above must be saved using python pickle module.\n",
        "\n",
        "    :param dataset_str: Dataset name\n",
        "    :return: All data input files loaded (as well the training/test data).\n",
        "    \"\"\"\n",
        "    names = ['x', 'y', 'tx', 'ty', 'allx', 'ally', 'graph']\n",
        "    objects = []\n",
        "    for i in range(len(names)):\n",
        "        with open(\"/content/drive/MyDrive/Final Project/GFCN-master/GFCN/data/ind.{}.{}\".format(dataset_str, names[i]), 'rb') as f:\n",
        "            if sys.version_info > (3, 0):\n",
        "                objects.append(pkl.load(f, encoding='latin1'))\n",
        "            else:\n",
        "                objects.append(pkl.load(f))\n",
        "#/content/drive/MyDrive/Final Project/GFCN-master/GFCN/data\n",
        "    x, y, tx, ty, allx, ally, graph = tuple(objects)\n",
        "    test_idx_reorder = parse_index_file(\"/content/drive/MyDrive/Final Project/GFCN-master/GFCN/data/ind.{}.test.index\".format(dataset_str))\n",
        "    test_idx_range = np.sort(test_idx_reorder)\n",
        "\n",
        "    if dataset_str == 'citeseer':\n",
        "        test_idx_range_full = range(min(test_idx_reorder), max(test_idx_reorder)+1)\n",
        "        tx_extended = sp.lil_matrix((len(test_idx_range_full), x.shape[1]))\n",
        "        tx_extended[test_idx_range-min(test_idx_range), :] = tx\n",
        "        tx = tx_extended\n",
        "        ty_extended = np.zeros((len(test_idx_range_full), y.shape[1]))\n",
        "        ty_extended[test_idx_range-min(test_idx_range), :] = ty\n",
        "        ty = ty_extended\n",
        "\n",
        "\n",
        "    features = sp.vstack((allx, tx)).tolil()\n",
        "    features[test_idx_reorder, :] = features[test_idx_range, :]\n",
        "    features = sklearn.preprocessing.scale(np.array(features.todense()))\n",
        "\n",
        "    adj = nx.adjacency_matrix(nx.from_dict_of_lists(graph))\n",
        "    adj =normalize(adj)\n",
        "\n",
        "    labels = np.vstack((ally, ty)).astype(np.int32)\n",
        "    labels[test_idx_reorder, :] = labels[test_idx_range, :]\n",
        "\n",
        "    size_classes = np.sum(labels, axis = 0)\n",
        "    abnormal_class = np.argmin(size_classes)\n",
        "\n",
        "    labels = np.argmax(labels, axis=1)\n",
        "    labels = np.where(labels == abnormal_class,0,1)\n",
        "\n",
        "    idx = np.array(range(len(labels)))\n",
        "    np.random.shuffle(idx)\n",
        "    idx_normal = idx[idx != 0]  # Normal classes index\n",
        "\n",
        "    num_node = adj.shape[0]\n",
        "    num_train = int(num_node * 0.1)\n",
        "    num_val = int(num_node * 0.01)\n",
        "    all_idx = list(range(num_node))\n",
        "    idx_train = all_idx[ : num_train]\n",
        "    idx_val = all_idx[num_train : num_train + num_val]\n",
        "    idx_test = all_idx[num_train + num_val : ]\n",
        "\n",
        "\n",
        "    features = torch.FloatTensor(features)\n",
        "    labels = torch.LongTensor(labels)\n",
        "    adj = sparse_mx_to_torch_sparse_tensor(adj)\n",
        "    idx_train = torch.LongTensor(idx_train)\n",
        "    idx_test = torch.LongTensor(idx_test)\n",
        "    idx_val = torch.LongTensor(idx_val)\n",
        "\n",
        "    return adj, features, labels, idx_train, idx_test, idx_val\n"
      ],
      "metadata": {
        "id": "wODmHcF7Yk0f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Train"
      ],
      "metadata": {
        "id": "d8ddwPmTZ1KQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Filtering out Jupyter arguments\n",
        "args_to_keep = ['--no-cuda', '--fastmode', '--seed', '--epochs', '--lr', '--weight_decay', '--hidden', '--dropout', '--alpha', '--beta']\n",
        "args = [arg for arg in sys.argv if any(arg.startswith(a) for a in args_to_keep)]\n",
        "\n",
        "# Training settings\n",
        "parser = argparse.ArgumentParser()\n",
        "parser.add_argument('--no-cuda', action='store_true', default=False,\n",
        "                    help='Disables CUDA training.')\n",
        "parser.add_argument('--fastmode', action='store_true', default=False,\n",
        "                    help='Validate during training pass.')\n",
        "parser.add_argument('--seed', type=int, default=123, help='Random seed.')\n",
        "parser.add_argument('--epochs', type=int, default=200,\n",
        "                    help='Number of epochs to train.')\n",
        "parser.add_argument('--lr', type=float, default=0.1,\n",
        "                    help='Initial learning rate.')\n",
        "parser.add_argument('--weight_decay', type=float, default=0,\n",
        "                    help='Weight decay (L2 loss on parameters).')\n",
        "parser.add_argument('--hidden', type=int, default=128,\n",
        "                    help='Number of hidden units.')\n",
        "parser.add_argument('--dropout', type=float, default=0.5,\n",
        "                    help='Dropout rate (1 - keep probability).')\n",
        "parser.add_argument('--alpha', type=float, default=5,\n",
        "                    help='class-balanced parameter.')\n",
        "parser.add_argument('--beta', type=float, default=1e-1,\n",
        "                    help='l2 regularization parameter).')\n",
        "\n",
        "args = parser.parse_args(args)\n",
        "args.cuda = not args.no_cuda and torch.cuda.is_available()\n",
        "\n",
        "np.random.seed(args.seed)\n",
        "torch.manual_seed(args.seed)\n",
        "if args.cuda:\n",
        "    torch.cuda.manual_seed(args.seed)\n",
        "\n",
        "# Load data (Assuming load_data function is defined elsewhere)\n",
        "adj, features, labels, idx_train, idx_test, idx_val = load_data(\"pubmed\")  #cora, citeseer, pubmed\n",
        "\n",
        "# Model and optimizer (Assuming GCN class is defined elsewhere)\n",
        "model = GCN(nfeat=features.shape[1],\n",
        "            nhid=args.hidden,\n",
        "            nclass=labels.max().item() + 1,\n",
        "            dropout=args.dropout)\n",
        "optimizer = optim.Adam(model.parameters(),\n",
        "                       lr=args.lr, weight_decay=args.weight_decay)\n",
        "\n",
        "if args.cuda:\n",
        "    model.cuda()\n",
        "    features = features.cuda()\n",
        "    adj = adj.cuda()\n",
        "    labels = labels.cuda()\n",
        "    idx_train = idx_train.cuda()\n",
        "    idx_val = idx_val.cuda()\n",
        "    idx_test = idx_test.cuda()\n",
        "\n",
        "weights = [args.alpha, 1]\n",
        "class_weights = torch.FloatTensor(weights)\n",
        "\n",
        "def train(epoch):\n",
        "    t = time.time()\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    output, w1, w2, w3, w4 = model(features, adj)\n",
        "\n",
        "    w1 = torch.pow(torch.norm(w1), 2)\n",
        "    w2 = torch.pow(torch.norm(w2), 2)\n",
        "    w3 = torch.pow(torch.norm(w3), 2)\n",
        "    w4 = torch.pow(torch.norm(w4), 2)\n",
        "    l2_reg = w1 + w2 + w3 + w4\n",
        "\n",
        "    loss = torch.nn.CrossEntropyLoss(weight=class_weights)\n",
        "    loss_train = loss(output[idx_train], labels[idx_train]) + args.beta*l2_reg\n",
        "    loss_train.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if not args.fastmode:\n",
        "        # Evaluate validation set performance separately,\n",
        "        # deactivates dropout during validation run.\n",
        "        model.eval()\n",
        "        output, w1, w2, w3, w4 = model(features, adj)\n",
        "\n",
        "    loss_val = loss(output[idx_val], labels[idx_val]) + args.beta*l2_reg\n",
        "    print('Epoch: {:04d}'.format(epoch + 1),\n",
        "          'loss_train: {:.4f}'.format(loss_train.item()),\n",
        "          'loss_val: {:.4f}'.format(loss_val.item()),\n",
        "          'time: {:.4f}s'.format(time.time() - t))\n",
        "\n",
        "def test():\n",
        "    model.eval()\n",
        "    output, _, _, _, _ = model(features, adj)\n",
        "\n",
        "    loss = torch.nn.CrossEntropyLoss(weight = class_weights)\n",
        "    loss_test = loss(output[idx_test], labels[idx_test])\n",
        "\n",
        "    scores = F.softmax(output, dim=1)\n",
        "    fpr, tpr, t = roc_curve(labels[idx_test].detach().numpy(), scores[idx_test, 0].detach().numpy(), pos_label = 0)\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "    return roc_auc\n",
        "\n",
        "# Train model\n",
        "t_total = time.time()\n",
        "max_auc = 0\n",
        "for epoch in range(args.epochs):\n",
        "    train(epoch)\n",
        "    roc_auc = test()\n",
        "    if roc_auc > max_auc:\n",
        "        max_auc = roc_auc\n",
        "AUC = max_auc\n",
        "print('AUC: {:04f}'.format(AUC))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n3im0jVlZyXt",
        "outputId": "a3af0fb8-2231-489f-ded8-8f4635d741e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-12-f923441b00ba>:52: DeprecationWarning: Please use `csr_matrix` from the `scipy.sparse` namespace, the `scipy.sparse.csr` namespace is deprecated.\n",
            "  objects.append(pkl.load(f, encoding='latin1'))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_data.py:240: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_data.py:259: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. \n",
            "  warnings.warn(\n",
            "<ipython-input-12-f923441b00ba>:15: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:621.)\n",
            "  return torch.sparse.FloatTensor(indices, values, shape)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0001 loss_train: 62.2663 loss_val: 71.3841 time: 1.0726s\n",
            "Epoch: 0002 loss_train: 118.2968 loss_val: 113.6076 time: 1.0804s\n",
            "Epoch: 0003 loss_train: 85.8701 loss_val: 80.6220 time: 1.1385s\n",
            "Epoch: 0004 loss_train: 48.2872 loss_val: 46.4118 time: 1.1745s\n",
            "Epoch: 0005 loss_train: 36.7624 loss_val: 37.0893 time: 0.9658s\n",
            "Epoch: 0006 loss_train: 36.9681 loss_val: 37.8715 time: 0.8581s\n",
            "Epoch: 0007 loss_train: 35.5215 loss_val: 35.4922 time: 1.1151s\n",
            "Epoch: 0008 loss_train: 30.2127 loss_val: 30.2288 time: 0.9429s\n",
            "Epoch: 0009 loss_train: 25.1002 loss_val: 25.4411 time: 0.8694s\n",
            "Epoch: 0010 loss_train: 21.9964 loss_val: 22.4839 time: 0.6892s\n",
            "Epoch: 0011 loss_train: 20.2803 loss_val: 19.9656 time: 0.6421s\n",
            "Epoch: 0012 loss_train: 18.4419 loss_val: 18.7817 time: 0.5717s\n",
            "Epoch: 0013 loss_train: 17.5624 loss_val: 18.0375 time: 0.5570s\n",
            "Epoch: 0014 loss_train: 17.0243 loss_val: 16.8227 time: 0.5456s\n",
            "Epoch: 0015 loss_train: 15.3597 loss_val: 15.5449 time: 0.5496s\n",
            "Epoch: 0016 loss_train: 13.8409 loss_val: 13.9256 time: 0.5312s\n",
            "Epoch: 0017 loss_train: 12.3508 loss_val: 12.2391 time: 0.5244s\n",
            "Epoch: 0018 loss_train: 10.9437 loss_val: 10.9877 time: 0.5483s\n",
            "Epoch: 0019 loss_train: 9.8941 loss_val: 9.9590 time: 0.5805s\n",
            "Epoch: 0020 loss_train: 9.0772 loss_val: 9.1013 time: 0.8277s\n",
            "Epoch: 0021 loss_train: 8.2601 loss_val: 8.3016 time: 0.8293s\n",
            "Epoch: 0022 loss_train: 7.4358 loss_val: 7.4298 time: 0.8213s\n",
            "Epoch: 0023 loss_train: 6.6452 loss_val: 6.6608 time: 0.8260s\n",
            "Epoch: 0024 loss_train: 6.0224 loss_val: 6.0369 time: 0.6362s\n",
            "Epoch: 0025 loss_train: 5.4589 loss_val: 5.4977 time: 0.5256s\n",
            "Epoch: 0026 loss_train: 5.0249 loss_val: 5.0624 time: 0.5335s\n",
            "Epoch: 0027 loss_train: 4.6608 loss_val: 4.7866 time: 0.5569s\n",
            "Epoch: 0028 loss_train: 4.2993 loss_val: 4.6477 time: 0.5572s\n",
            "Epoch: 0029 loss_train: 3.9919 loss_val: 4.2448 time: 0.5309s\n",
            "Epoch: 0030 loss_train: 3.6460 loss_val: 3.7648 time: 0.5656s\n",
            "Epoch: 0031 loss_train: 3.2754 loss_val: 3.4083 time: 0.5723s\n",
            "Epoch: 0032 loss_train: 2.9989 loss_val: 3.0978 time: 0.5420s\n",
            "Epoch: 0033 loss_train: 2.7107 loss_val: 2.8294 time: 0.5424s\n",
            "Epoch: 0034 loss_train: 2.4473 loss_val: 2.5727 time: 0.5467s\n",
            "Epoch: 0035 loss_train: 2.2200 loss_val: 2.3534 time: 0.5393s\n",
            "Epoch: 0036 loss_train: 2.0472 loss_val: 2.1535 time: 0.5362s\n",
            "Epoch: 0037 loss_train: 1.8882 loss_val: 1.9750 time: 0.7267s\n",
            "Epoch: 0038 loss_train: 1.7410 loss_val: 1.8351 time: 0.7988s\n",
            "Epoch: 0039 loss_train: 1.6195 loss_val: 1.6980 time: 0.8334s\n",
            "Epoch: 0040 loss_train: 1.4777 loss_val: 1.5612 time: 0.8420s\n",
            "Epoch: 0041 loss_train: 1.3794 loss_val: 1.4356 time: 0.7322s\n",
            "Epoch: 0042 loss_train: 1.2946 loss_val: 1.3366 time: 0.5362s\n",
            "Epoch: 0043 loss_train: 1.2133 loss_val: 1.2493 time: 0.5459s\n",
            "Epoch: 0044 loss_train: 1.1485 loss_val: 1.1691 time: 0.6005s\n",
            "Epoch: 0045 loss_train: 1.0560 loss_val: 1.0969 time: 0.5335s\n",
            "Epoch: 0046 loss_train: 0.9780 loss_val: 1.0280 time: 0.5416s\n",
            "Epoch: 0047 loss_train: 0.9180 loss_val: 0.9597 time: 0.6206s\n",
            "Epoch: 0048 loss_train: 0.8497 loss_val: 0.9005 time: 0.5282s\n",
            "Epoch: 0049 loss_train: 0.8150 loss_val: 0.8348 time: 0.5326s\n",
            "Epoch: 0050 loss_train: 0.7583 loss_val: 0.7876 time: 0.5949s\n",
            "Epoch: 0051 loss_train: 0.7312 loss_val: 0.7539 time: 0.5493s\n",
            "Epoch: 0052 loss_train: 0.6888 loss_val: 0.7266 time: 0.5415s\n",
            "Epoch: 0053 loss_train: 0.6444 loss_val: 0.6912 time: 0.5378s\n",
            "Epoch: 0054 loss_train: 0.6238 loss_val: 0.6674 time: 0.5673s\n",
            "Epoch: 0055 loss_train: 0.5993 loss_val: 0.6450 time: 0.8328s\n",
            "Epoch: 0056 loss_train: 0.5669 loss_val: 0.6214 time: 0.8270s\n",
            "Epoch: 0057 loss_train: 0.5521 loss_val: 0.5875 time: 0.8130s\n",
            "Epoch: 0058 loss_train: 0.5306 loss_val: 0.5681 time: 0.8036s\n",
            "Epoch: 0059 loss_train: 0.5083 loss_val: 0.5535 time: 0.5307s\n",
            "Epoch: 0060 loss_train: 0.4837 loss_val: 0.5486 time: 0.5291s\n",
            "Epoch: 0061 loss_train: 0.4694 loss_val: 0.5327 time: 0.5408s\n",
            "Epoch: 0062 loss_train: 0.4506 loss_val: 0.5235 time: 0.5190s\n",
            "Epoch: 0063 loss_train: 0.4506 loss_val: 0.5171 time: 0.5115s\n",
            "Epoch: 0064 loss_train: 0.4310 loss_val: 0.5039 time: 0.5494s\n",
            "Epoch: 0065 loss_train: 0.4246 loss_val: 0.4884 time: 0.5345s\n",
            "Epoch: 0066 loss_train: 0.4191 loss_val: 0.4729 time: 0.5151s\n",
            "Epoch: 0067 loss_train: 0.4027 loss_val: 0.4541 time: 0.5441s\n",
            "Epoch: 0068 loss_train: 0.3925 loss_val: 0.4321 time: 0.5385s\n",
            "Epoch: 0069 loss_train: 0.3963 loss_val: 0.4023 time: 0.5404s\n",
            "Epoch: 0070 loss_train: 0.3845 loss_val: 0.3943 time: 0.5715s\n",
            "Epoch: 0071 loss_train: 0.3831 loss_val: 0.3972 time: 0.5523s\n",
            "Epoch: 0072 loss_train: 0.3648 loss_val: 0.4063 time: 0.6716s\n",
            "Epoch: 0073 loss_train: 0.3794 loss_val: 0.4097 time: 0.8159s\n",
            "Epoch: 0074 loss_train: 0.3707 loss_val: 0.4175 time: 0.8485s\n",
            "Epoch: 0075 loss_train: 0.3706 loss_val: 0.4235 time: 0.8244s\n",
            "Epoch: 0076 loss_train: 0.3661 loss_val: 0.4146 time: 0.8162s\n",
            "Epoch: 0077 loss_train: 0.3512 loss_val: 0.4084 time: 0.5421s\n",
            "Epoch: 0078 loss_train: 0.3596 loss_val: 0.3976 time: 0.5255s\n",
            "Epoch: 0079 loss_train: 0.3524 loss_val: 0.3917 time: 0.5447s\n",
            "Epoch: 0080 loss_train: 0.3574 loss_val: 0.3991 time: 0.5389s\n",
            "Epoch: 0081 loss_train: 0.3545 loss_val: 0.3990 time: 0.5206s\n",
            "Epoch: 0082 loss_train: 0.3596 loss_val: 0.3908 time: 0.5293s\n",
            "Epoch: 0083 loss_train: 0.3472 loss_val: 0.3989 time: 0.5461s\n",
            "Epoch: 0084 loss_train: 0.3516 loss_val: 0.4090 time: 0.5374s\n",
            "Epoch: 0085 loss_train: 0.3512 loss_val: 0.3976 time: 0.5847s\n",
            "Epoch: 0086 loss_train: 0.3448 loss_val: 0.4027 time: 0.5440s\n",
            "Epoch: 0087 loss_train: 0.3538 loss_val: 0.4034 time: 0.5391s\n",
            "Epoch: 0088 loss_train: 0.3371 loss_val: 0.3997 time: 0.5239s\n",
            "Epoch: 0089 loss_train: 0.3461 loss_val: 0.3787 time: 0.5887s\n",
            "Epoch: 0090 loss_train: 0.3372 loss_val: 0.3818 time: 0.8326s\n",
            "Epoch: 0091 loss_train: 0.3381 loss_val: 0.3911 time: 0.8897s\n",
            "Epoch: 0092 loss_train: 0.3405 loss_val: 0.3744 time: 0.8868s\n",
            "Epoch: 0093 loss_train: 0.3507 loss_val: 0.3646 time: 0.8208s\n",
            "Epoch: 0094 loss_train: 0.3378 loss_val: 0.3730 time: 0.5932s\n",
            "Epoch: 0095 loss_train: 0.3478 loss_val: 0.3769 time: 0.5270s\n",
            "Epoch: 0096 loss_train: 0.3573 loss_val: 0.3823 time: 0.5236s\n",
            "Epoch: 0097 loss_train: 0.3375 loss_val: 0.3910 time: 0.5341s\n",
            "Epoch: 0098 loss_train: 0.3317 loss_val: 0.3981 time: 0.5147s\n",
            "Epoch: 0099 loss_train: 0.3482 loss_val: 0.3695 time: 0.5332s\n",
            "Epoch: 0100 loss_train: 0.3387 loss_val: 0.3746 time: 0.5875s\n",
            "Epoch: 0101 loss_train: 0.3478 loss_val: 0.3751 time: 0.5338s\n",
            "Epoch: 0102 loss_train: 0.3405 loss_val: 0.3711 time: 0.5305s\n",
            "Epoch: 0103 loss_train: 0.3345 loss_val: 0.3670 time: 0.5831s\n",
            "Epoch: 0104 loss_train: 0.3399 loss_val: 0.3675 time: 0.5244s\n",
            "Epoch: 0105 loss_train: 0.3487 loss_val: 0.3787 time: 0.5878s\n",
            "Epoch: 0106 loss_train: 0.3413 loss_val: 0.3939 time: 0.5958s\n",
            "Epoch: 0107 loss_train: 0.3324 loss_val: 0.3995 time: 0.5493s\n",
            "Epoch: 0108 loss_train: 0.3414 loss_val: 0.3883 time: 0.8103s\n",
            "Epoch: 0109 loss_train: 0.3352 loss_val: 0.3848 time: 0.8241s\n",
            "Epoch: 0110 loss_train: 0.3406 loss_val: 0.3803 time: 0.8083s\n",
            "Epoch: 0111 loss_train: 0.3385 loss_val: 0.3587 time: 0.8293s\n",
            "Epoch: 0112 loss_train: 0.3333 loss_val: 0.3626 time: 0.5329s\n",
            "Epoch: 0113 loss_train: 0.3373 loss_val: 0.3692 time: 0.5238s\n",
            "Epoch: 0114 loss_train: 0.3386 loss_val: 0.3745 time: 0.5486s\n",
            "Epoch: 0115 loss_train: 0.3468 loss_val: 0.3976 time: 0.5428s\n",
            "Epoch: 0116 loss_train: 0.3384 loss_val: 0.4129 time: 0.5193s\n",
            "Epoch: 0117 loss_train: 0.3424 loss_val: 0.3948 time: 0.5074s\n",
            "Epoch: 0118 loss_train: 0.3468 loss_val: 0.3815 time: 0.5432s\n",
            "Epoch: 0119 loss_train: 0.3452 loss_val: 0.3707 time: 0.5427s\n",
            "Epoch: 0120 loss_train: 0.3429 loss_val: 0.3673 time: 0.5309s\n",
            "Epoch: 0121 loss_train: 0.3538 loss_val: 0.3671 time: 0.5491s\n",
            "Epoch: 0122 loss_train: 0.3365 loss_val: 0.3920 time: 0.5462s\n",
            "Epoch: 0123 loss_train: 0.3434 loss_val: 0.3942 time: 0.5379s\n",
            "Epoch: 0124 loss_train: 0.3473 loss_val: 0.3934 time: 0.5230s\n",
            "Epoch: 0125 loss_train: 0.3405 loss_val: 0.3975 time: 0.6955s\n",
            "Epoch: 0126 loss_train: 0.3621 loss_val: 0.3823 time: 0.8031s\n",
            "Epoch: 0127 loss_train: 0.3401 loss_val: 0.3845 time: 0.8161s\n",
            "Epoch: 0128 loss_train: 0.3443 loss_val: 0.3717 time: 0.8189s\n",
            "Epoch: 0129 loss_train: 0.3463 loss_val: 0.3632 time: 0.7627s\n",
            "Epoch: 0130 loss_train: 0.3552 loss_val: 0.3572 time: 0.5384s\n",
            "Epoch: 0131 loss_train: 0.3485 loss_val: 0.3843 time: 0.5237s\n",
            "Epoch: 0132 loss_train: 0.3405 loss_val: 0.3658 time: 0.5149s\n",
            "Epoch: 0133 loss_train: 0.3484 loss_val: 0.3789 time: 0.5226s\n",
            "Epoch: 0134 loss_train: 0.3536 loss_val: 0.4078 time: 0.5289s\n",
            "Epoch: 0135 loss_train: 0.3502 loss_val: 0.3989 time: 0.5220s\n",
            "Epoch: 0136 loss_train: 0.3331 loss_val: 0.4006 time: 0.5463s\n",
            "Epoch: 0137 loss_train: 0.3431 loss_val: 0.4128 time: 0.5884s\n",
            "Epoch: 0138 loss_train: 0.3440 loss_val: 0.4039 time: 0.5403s\n",
            "Epoch: 0139 loss_train: 0.3397 loss_val: 0.3970 time: 0.5305s\n",
            "Epoch: 0140 loss_train: 0.3501 loss_val: 0.3835 time: 0.5179s\n",
            "Epoch: 0141 loss_train: 0.3534 loss_val: 0.3810 time: 0.5258s\n",
            "Epoch: 0142 loss_train: 0.3474 loss_val: 0.3597 time: 0.5237s\n",
            "Epoch: 0143 loss_train: 0.3401 loss_val: 0.3700 time: 0.6866s\n",
            "Epoch: 0144 loss_train: 0.3518 loss_val: 0.3896 time: 0.8007s\n",
            "Epoch: 0145 loss_train: 0.3512 loss_val: 0.3738 time: 0.7951s\n",
            "Epoch: 0146 loss_train: 0.3490 loss_val: 0.3760 time: 0.7896s\n",
            "Epoch: 0147 loss_train: 0.3516 loss_val: 0.3775 time: 0.8426s\n",
            "Epoch: 0148 loss_train: 0.3486 loss_val: 0.3763 time: 0.5103s\n",
            "Epoch: 0149 loss_train: 0.3448 loss_val: 0.3700 time: 0.5051s\n",
            "Epoch: 0150 loss_train: 0.3451 loss_val: 0.3804 time: 0.5370s\n",
            "Epoch: 0151 loss_train: 0.3448 loss_val: 0.3779 time: 0.5191s\n",
            "Epoch: 0152 loss_train: 0.3560 loss_val: 0.4169 time: 0.5351s\n",
            "Epoch: 0153 loss_train: 0.3461 loss_val: 0.4257 time: 0.5415s\n",
            "Epoch: 0154 loss_train: 0.3444 loss_val: 0.4202 time: 0.4961s\n",
            "Epoch: 0155 loss_train: 0.3382 loss_val: 0.4314 time: 0.5161s\n",
            "Epoch: 0156 loss_train: 0.3495 loss_val: 0.4056 time: 0.6229s\n",
            "Epoch: 0157 loss_train: 0.3447 loss_val: 0.3907 time: 0.5372s\n",
            "Epoch: 0158 loss_train: 0.3454 loss_val: 0.3872 time: 0.5268s\n",
            "Epoch: 0159 loss_train: 0.3364 loss_val: 0.3816 time: 0.5141s\n",
            "Epoch: 0160 loss_train: 0.3451 loss_val: 0.3682 time: 0.5330s\n",
            "Epoch: 0161 loss_train: 0.3406 loss_val: 0.3761 time: 0.7212s\n",
            "Epoch: 0162 loss_train: 0.3481 loss_val: 0.4008 time: 0.8096s\n",
            "Epoch: 0163 loss_train: 0.3582 loss_val: 0.3901 time: 0.8069s\n",
            "Epoch: 0164 loss_train: 0.3555 loss_val: 0.4078 time: 0.8078s\n",
            "Epoch: 0165 loss_train: 0.3465 loss_val: 0.4158 time: 0.6324s\n",
            "Epoch: 0166 loss_train: 0.3471 loss_val: 0.3844 time: 0.5202s\n",
            "Epoch: 0167 loss_train: 0.3488 loss_val: 0.3924 time: 0.5031s\n",
            "Epoch: 0168 loss_train: 0.3488 loss_val: 0.3951 time: 0.5328s\n",
            "Epoch: 0169 loss_train: 0.3478 loss_val: 0.3463 time: 0.5311s\n",
            "Epoch: 0170 loss_train: 0.3420 loss_val: 0.3633 time: 0.5255s\n",
            "Epoch: 0171 loss_train: 0.3542 loss_val: 0.4167 time: 0.5335s\n",
            "Epoch: 0172 loss_train: 0.3836 loss_val: 0.3753 time: 0.5299s\n",
            "Epoch: 0173 loss_train: 0.3613 loss_val: 0.3952 time: 0.5201s\n",
            "Epoch: 0174 loss_train: 0.3521 loss_val: 0.4612 time: 0.5474s\n",
            "Epoch: 0175 loss_train: 0.3619 loss_val: 0.4209 time: 0.5424s\n",
            "Epoch: 0176 loss_train: 0.3570 loss_val: 0.4369 time: 0.5394s\n",
            "Epoch: 0177 loss_train: 0.3552 loss_val: 0.4484 time: 0.5255s\n",
            "Epoch: 0178 loss_train: 0.3622 loss_val: 0.3949 time: 0.5398s\n",
            "Epoch: 0179 loss_train: 0.3532 loss_val: 0.3951 time: 0.7591s\n",
            "Epoch: 0180 loss_train: 0.3486 loss_val: 0.4053 time: 0.7925s\n",
            "Epoch: 0181 loss_train: 0.3523 loss_val: 0.3594 time: 0.8087s\n",
            "Epoch: 0182 loss_train: 0.3559 loss_val: 0.3659 time: 0.8067s\n",
            "Epoch: 0183 loss_train: 0.3478 loss_val: 0.3974 time: 0.7292s\n",
            "Epoch: 0184 loss_train: 0.3636 loss_val: 0.3817 time: 0.5403s\n",
            "Epoch: 0185 loss_train: 0.3575 loss_val: 0.4035 time: 0.5269s\n",
            "Epoch: 0186 loss_train: 0.3572 loss_val: 0.4274 time: 0.5120s\n",
            "Epoch: 0187 loss_train: 0.3583 loss_val: 0.4009 time: 0.5366s\n",
            "Epoch: 0188 loss_train: 0.3609 loss_val: 0.3940 time: 0.5381s\n",
            "Epoch: 0189 loss_train: 0.3578 loss_val: 0.3989 time: 0.5170s\n",
            "Epoch: 0190 loss_train: 0.3495 loss_val: 0.3841 time: 0.4930s\n",
            "Epoch: 0191 loss_train: 0.3628 loss_val: 0.3854 time: 0.5441s\n",
            "Epoch: 0192 loss_train: 0.3553 loss_val: 0.3943 time: 0.5323s\n",
            "Epoch: 0193 loss_train: 0.3412 loss_val: 0.4043 time: 0.5792s\n",
            "Epoch: 0194 loss_train: 0.3582 loss_val: 0.4207 time: 0.5348s\n",
            "Epoch: 0195 loss_train: 0.3498 loss_val: 0.4188 time: 0.5280s\n",
            "Epoch: 0196 loss_train: 0.3578 loss_val: 0.4209 time: 0.5336s\n",
            "Epoch: 0197 loss_train: 0.3515 loss_val: 0.4080 time: 0.7691s\n",
            "Epoch: 0198 loss_train: 0.3508 loss_val: 0.3880 time: 0.7702s\n",
            "Epoch: 0199 loss_train: 0.3528 loss_val: 0.3746 time: 0.8179s\n",
            "Epoch: 0200 loss_train: 0.3496 loss_val: 0.3846 time: 0.8176s\n",
            "AUC: 0.974643\n"
          ]
        }
      ]
    }
  ]
}